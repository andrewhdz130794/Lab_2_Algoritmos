# -*- coding: utf-8 -*-
"""gradiante.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_RAt5rgrfiVMHWptY9T8NuEj_q_sZwbU

# Problema #1
"""

import numpy as np

def gradient_descent(Q, c, x0, epsilon, N, alpha_strategy):
    """
    Implementación del algoritmo Gradient Descent para una función cuadrática.

    Parametrors:
    - Q: Matriz simétrica y definida positiva.
    - c: Vector de coeficientes.
    - x0: Punto inicial.
    - epsilon: Tolerancia para la norma del gradiente.
    - N: Número máximo de iteraciones.
    - alpha_strategy: Estrategia para el step size (exacto, constante, variable).

    Returns:
    - history: Lista con las iteraciones (k, x_k, p_k, ||∇f(x_k)||).
    """
    def gradient(x):
        return np.dot(Q, x) + c

    def exact_step_size(x, grad):
        return np.dot(grad, grad) / np.dot(np.dot(grad, Q), grad)

    x = x0
    history = []

    for k in range(N):
        grad = gradient(x)
        grad_norm = np.linalg.norm(grad)

        if grad_norm < epsilon:
            break

        if alpha_strategy == 'exact':
            alpha_k = exact_step_size(x, grad)
        elif alpha_strategy == 'constant':
            alpha_k = 0.1  # aqui se colocan los diferentes valores de a
        elif alpha_strategy == 'variable':
            alpha_k = 1 / (k + 1)
        else:
            raise ValueError("Invalid alpha_strategy. Choose from 'exact', 'constant', or 'variable'.")

        p_k = -grad
        x = x + alpha_k * p_k

        history.append((k, x, p_k, grad_norm))

    return history

# Aqui se ingresan los valores del parametro (como aparecen en el ejercicio)
Q = np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]])
c = np.array([1, 0, 1])
x0 = np.array([3, 5, 7])
epsilon = 1e-6
N = 30
for alpha_strategy in ['exact', 'constant', 'variable']:
    result = gradient_descent(Q, c, x0, epsilon, N, alpha_strategy)

    print(f"Resultados para alpha_strategy = {alpha_strategy}")
    print("k\t x_k\t\t p_k\t\t ||∇f(x_k)||")
    for k, x_k, p_k, grad_norm in result:
        print(f"{k}\t {x_k}\t {p_k}\t {grad_norm}")
    print("\n")

"""# Problema #2"""

import numpy as np

def rosenbrock_function(x):
    """Calcula el valor de la función de Rosenbrock."""
    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2

def rosenbrock_gradient(x):
    """Calcula el gradiente de la función de Rosenbrock."""
    dfdx1 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])
    dfdx2 = 200 * (x[1] - x[0]**2)
    return np.array([dfdx1, dfdx2])

def gradient_descent_rosenbrock(x0, alpha, epsilon, N):
    """
    Implementación del algoritmo Gradient Descent para la función de Rosenbrock.

    Parametros:
    - x0: Punto inicial.
    - alpha: Step size constante.
    - epsilon: Tolerancia para la norma del gradiente.
    - N: Número máximo de iteraciones.

    Returns:
    - history: Lista con las iteraciones (k, x_k, p_k, ||∇f(x_k)||).
    """
    x = x0
    history = []

    for k in range(N):
        grad = rosenbrock_gradient(x)
        grad_norm = np.linalg.norm(grad)

        if grad_norm < epsilon:
            break

        p_k = -grad
        x = x + alpha * p_k

        history.append((k, x.copy(), p_k.copy(), grad_norm))

    return history

# Aqui se colocan los paramertros que se desean evaliar del problema
x0 = np.array([0, 0])
alpha = 0.05
epsilon = 1e-8
N = 1000

# Ejecutar el algoritmo
result = gradient_descent_rosenbrock(x0, alpha, epsilon, N)

# Mostrar los resultados
print("Resultados para la función de Rosenbrock")
print("k\t x_k\t\t\t\t p_k\t\t\t ||∇f(x_k)||")
for k, x_k, p_k, grad_norm in result:
    print(f"{k}\t {x_k}\t {p_k}\t {grad_norm}")